{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f315d93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data berhasil disimpan ke dataset/data_bersih_sentimen.csv\n",
      "Akurasi: 0.8579881656804734\n",
      "Confusion Matrix:\n",
      " [[134   0]\n",
      " [ 24  11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.85      1.00      0.92       134\n",
      "     positif       1.00      0.31      0.48        35\n",
      "\n",
      "    accuracy                           0.86       169\n",
      "   macro avg       0.92      0.66      0.70       169\n",
      "weighted avg       0.88      0.86      0.83       169\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# === 1. Load dan filter data ===\n",
    "df = pd.read_csv('dataset/ikn.csv')\n",
    "keywords = ['ikn', 'nusantara', 'ibu kota', 'ibukota', 'pemindahan', 'perpindahan']\n",
    "filtered_df = df[df['full_text'].str.contains('|'.join(keywords), case=False)].copy()\n",
    "\n",
    "filtered_df = filtered_df[filtered_df['full_text'].str.strip() != '']\n",
    "filtered_df = filtered_df.drop_duplicates(subset='full_text')\n",
    "filtered_df['full_text'] = filtered_df['full_text'].str.replace(r'http\\S+', '', regex=True)\n",
    "filtered_df['word_count'] = filtered_df['full_text'].apply(lambda x: len(x.split()))\n",
    "filtered_df = filtered_df[filtered_df['word_count'] >= 5]\n",
    "filtered_df.drop(columns=['word_count'], inplace=True)\n",
    "\n",
    "# === 2. Preprocessing ===\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "tambahan_stop = {\n",
    "    'gw', 'gue', 'gua', 'lu', 'loe', 'lo', 'elu', 'nya', 'ya', 'aja', 'sih', 'lah', 'deh', 'dong',\n",
    "    'kok', 'nih', 'tuh', 'lagi', 'kayak', 'gak', 'ga', 'nggak', 'ngga', 'yg', 'yang', 'saya', 'kamu'\n",
    "}\n",
    "stop_words.update(tambahan_stop)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\bgw\\b|\\bgue\\b|\\bgua\\b', 'saya', text)\n",
    "    text = re.sub(r'\\blu\\b|\\bloe\\b|\\belo\\b|\\belu\\b', 'kamu', text)\n",
    "    text = re.sub(r'\\bnggak\\b|\\bngga\\b|\\bga\\b|\\bgak\\b', 'tidak', text)\n",
    "    text = stemmer.stem(text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "filtered_df['cleaned'] = filtered_df['full_text'].apply(clean_text)\n",
    "\n",
    "# === 2.1 Perkaya dan Gabungkan Kamus Kata Dasar ===\n",
    "with open('sastrawi/kata-dasar.txt', 'r', encoding='utf-8') as f:\n",
    "    kamus_sastrawi = set(f.read().splitlines())\n",
    "\n",
    "pos_words = [\n",
    "    'baik', 'bagus', 'mendukung', 'hebat', 'mantap', 'setuju',\n",
    "    'top', 'luar biasa', 'sip', 'apresiasi', 'bangga',\n",
    "    'kemajuan', 'ramah', 'berkelanjutan', 'peluang', 'strategis',\n",
    "    'pertumbuhan', 'investasi', 'unggulan', 'mendorong', 'semangat',\n",
    "    'kondusif', 'cocok', 'positif', 'cerah', 'maju', 'bermanfaat',\n",
    "    'berkembang', 'terdepan', 'mendorong', 'sukses', 'membangun'\n",
    "]\n",
    "neg_words = [\n",
    "    'buruk', 'jelek', 'korupsi', 'parah', 'gagal', 'tolak',\n",
    "    'tidak setuju', 'menolak', 'ancur', 'bubar', 'kritik', 'macet',\n",
    "    'tidak ramah', 'tidak layak', 'tidak manusiawi', 'tidak cocok',\n",
    "    'bohong', 'salah', 'mundur', 'krisis', 'masalah', 'keluhan',\n",
    "    'cacat', 'curang', 'konflik', 'resah'\n",
    "]\n",
    "\n",
    "pos_words_stemmed = [stemmer.stem(w.lower()) for w in pos_words]\n",
    "neg_words_stemmed = [stemmer.stem(w.lower()) for w in neg_words]\n",
    "\n",
    "kamus_kata = kamus_sastrawi.union(set(pos_words_stemmed)).union(set(neg_words_stemmed))\n",
    "\n",
    "def hapus_kata_non_kamus(text):\n",
    "    filtered_words = [word for word in text.split() if word in kamus_kata]\n",
    "    if len(filtered_words) == 0:\n",
    "        return text\n",
    "    else:\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "filtered_df['final_cleaned'] = filtered_df['cleaned'].apply(hapus_kata_non_kamus)\n",
    "\n",
    "# === 3. Sentiment label dari fungsi simple_sentiment ===\n",
    "def simple_sentiment(text):\n",
    "    score = 0\n",
    "    for word in text.split():\n",
    "        if word in pos_words_stemmed:\n",
    "            score += 1\n",
    "        elif word in neg_words_stemmed:\n",
    "            score -= 1\n",
    "    return 'positif' if score > 0 else 'negatif'\n",
    "\n",
    "filtered_df['sentiment'] = filtered_df['final_cleaned'].apply(simple_sentiment)\n",
    "\n",
    "# === 4. Simpan data bersih ===\n",
    "filtered_df[['full_text', 'final_cleaned', 'sentiment']].to_csv('dataset/data_bersih_sentimen.csv', index=False)\n",
    "print(\"âœ… Data berhasil disimpan ke dataset/data_bersih_sentimen.csv\")\n",
    "\n",
    "# === 5. Label encoding ===\n",
    "le = LabelEncoder()\n",
    "filtered_df['label'] = le.fit_transform(filtered_df['sentiment'])\n",
    "\n",
    "# === 6. TF-IDF + Naive Bayes ===\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X = filtered_df['final_cleaned']\n",
    "y = filtered_df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# === 7. Evaluasi model ===\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "labels = le.transform(['negatif', 'positif'])\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred, labels=labels))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, labels=labels, target_names=['negatif', 'positif'], zero_division=1))\n",
    "\n",
    "# === 8. Visualisasi distribusi sentimen ===\n",
    "filtered_df['sentiment'].value_counts().plot(kind='bar', color=['red', 'green'])\n",
    "plt.title('Distribusi Sentimen')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b0a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data berhasil disimpan ke tabel MySQL: 'dataset', 'preprocessing'\n",
      "ðŸ“Œ Contoh data hasil pembersihan dan sentimen:\n",
      "   id                                           stemming sentimen\n",
      "0   1  [\"baca\", \"baca\", \"dalam\", \"ikn\", \"ilustrasi\", ...  negatif\n",
      "1   2  [\"cabang\", \"dinasti\", \"dinasti\", \"dukung\", \"du...  positif\n",
      "2   3  [\"cermin\", \"identitas\", \"ikn\", \"indonesia\", \"k...  positif\n",
      "3   4  [\"bas\", \"ikn\", \"kota\", \"lanjut\", \"lingkung\", \"...  negatif\n",
      "4   5  [\"bahan\", \"bangun\", \"hidup\", \"ikn\", \"jasa\", \"j...  negatif\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import text\n",
    "import json\n",
    "\n",
    "# Download stopwords jika belum tersedia\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Koneksi ke MySQL via SQLAlchemy\n",
    "engine = create_engine(\"mysql+mysqlconnector://root:@localhost/ikn-app\")\n",
    "\n",
    "# Load dataset CSV\n",
    "df = pd.read_csv('dataset/ikn.csv')\n",
    "\n",
    "# Simpan ke tabel 'dataset'\n",
    "df.to_sql(name='dataset', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Filter hanya data dengan kata kunci\n",
    "keywords = ['ikn', 'nusantara', 'ibu kota', 'ibukota', 'pemindahan', 'perpindahan']\n",
    "filtered_df = df[df['full_text'].str.contains('|'.join(keywords), case=False, na=False)].copy()\n",
    "filtered_df = filtered_df[filtered_df['full_text'].str.strip() != '']\n",
    "filtered_df = filtered_df.drop_duplicates(subset='full_text')\n",
    "\n",
    "# Inisialisasi Stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Load kamus kata dasar\n",
    "kamus_path = 'sastrawi/kata-dasar.txt'\n",
    "with open(kamus_path, 'r', encoding='utf-8') as f:\n",
    "    kamus_sastrawi = set(word.strip() for word in f.readlines())\n",
    "\n",
    "# Kata penting\n",
    "kata_penting = {'ikn', 'nusantara', 'ibu', 'kota', 'ibukota', 'pemindahan', 'perpindahan'}\n",
    "\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "tambahan_stop = {\n",
    "    'gw', 'gue', 'gua', 'lu', 'loe', 'lo', 'elu', 'nya', 'ya', 'aja', 'sih', 'lah', 'deh', 'dong',\n",
    "    'kok', 'nih', 'tuh', 'lagi', 'kayak', 'gak', 'ga', 'nggak', 'ngga', 'yg', 'yang', 'saya', 'kamu'\n",
    "}\n",
    "stop_words.update(tambahan_stop)\n",
    "\n",
    "# Kata sentimen\n",
    "kata_positif = {'baik', 'bagus', 'maju', 'dukung', 'setuju', 'positif', 'indah', 'hebat'}\n",
    "kata_negatif = {'tidak', 'buruk', 'tolak', 'negatif', 'korup', 'jelek', 'hancur', 'bencana', 'rusak'}\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_steps(text):\n",
    "    # Bersihkan teks (hapus url, hashtag, mention, angka)\n",
    "    data_clean = re.sub(r'http\\S+|#\\w+|@\\w+|\\d+', '', text)\n",
    "    lower = data_clean.lower()\n",
    "    no_punct = lower.translate(str.maketrans('', '', string.punctuation))\n",
    "    replaced = re.sub(r'\\bgw\\b|\\bgue\\b|\\bgua\\b', 'saya', no_punct)\n",
    "    replaced = re.sub(r'\\blu\\b|\\bloe\\b|\\belo\\b|\\belu\\b', 'kamu', replaced)\n",
    "    replaced = re.sub(r'\\bnggak\\b|\\bngga\\b|\\bga\\b|\\bgak\\b', 'tidak', replaced)\n",
    "\n",
    "    tokens = replaced.split()\n",
    "\n",
    "    # Hapus stopword\n",
    "    tokens_stop_removed = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    # Gabungkan untuk stemming\n",
    "    joined_for_stem = ' '.join(tokens_stop_removed)\n",
    "    stemmed_text = stemmer.stem(joined_for_stem)\n",
    "\n",
    "    # Token hasil stemming\n",
    "    stemmed_tokens = stemmed_text.split()\n",
    "\n",
    "    # Filtering: hanya kata di kamus atau penting\n",
    "    tokens_filtered = [w for w in stemmed_tokens if w in kamus_sastrawi or w in kata_penting]\n",
    "\n",
    "    # Urutkan\n",
    "    tokens_sorted = sorted(tokens_filtered)\n",
    "\n",
    "    # Gabungkan hasil akhir\n",
    "    final_cleaned = ' '.join(tokens_sorted)\n",
    "\n",
    "    # Deteksi sentimen\n",
    "    sentimen = 'positif' if any(k in tokens_sorted for k in kata_positif) else (\n",
    "        'negatif' if any(k in tokens_sorted for k in kata_negatif) else 'negatif'\n",
    "    )\n",
    "\n",
    "    return pd.Series([\n",
    "        final_cleaned,\n",
    "        lower,\n",
    "        no_punct,\n",
    "        json.dumps(tokens, ensure_ascii=False),\n",
    "        json.dumps(tokens_stop_removed, ensure_ascii=False),\n",
    "        json.dumps(tokens_sorted, ensure_ascii=False),\n",
    "        sentimen\n",
    "    ])\n",
    "\n",
    "# Terapkan preprocessing\n",
    "processed = filtered_df['full_text'].apply(preprocess_steps)\n",
    "processed.columns = [\n",
    "    'data_clean',\n",
    "    'lowercasing',\n",
    "    'remove_punctuation',\n",
    "    'tokenizing',\n",
    "    'stopword',\n",
    "    'stemming',\n",
    "    'sentimen'\n",
    "]\n",
    "\n",
    "# Filter hasil akhir: hanya yang terdiri dari >= 3 kata\n",
    "processed['jumlah_kata'] = processed['data_clean'].str.split().str.len()\n",
    "processed = processed[processed['jumlah_kata'] >= 3].drop(columns=['jumlah_kata'])\n",
    "\n",
    "# Tambahkan kolom id sebagai primary key\n",
    "processed.reset_index(drop=True, inplace=True)\n",
    "processed.insert(0, 'id', processed.index + 1)\n",
    "\n",
    "# Simpan ke MySQL\n",
    "processed.to_sql(name='preprocessing', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Jadikan id sebagai PRIMARY KEY dan AUTO_INCREMENT\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"\"\"\n",
    "        ALTER TABLE preprocessing\n",
    "        MODIFY COLUMN id INT NOT NULL AUTO_INCREMENT,\n",
    "        ADD PRIMARY KEY (id);\n",
    "    \"\"\"))\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"âœ… Data berhasil disimpan ke tabel MySQL: 'dataset', 'preprocessing'\")\n",
    "print(\"ðŸ“Œ Contoh data hasil pembersihan dan sentimen:\")\n",
    "print(processed[['id', 'stemming', 'sentimen']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b84271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TF-IDF + Naive Bayes selesai.\n",
      "ðŸ“Œ Akurasi: 88.66%\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "            Pred_Pos  Pred_Neg\n",
      "Actual_Pos         1        22\n",
      "Actual_Neg         0       171\n",
      "\n",
      "ðŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.89      1.00      0.94       171\n",
      "     positif       1.00      0.04      0.08        23\n",
      "\n",
      "    accuracy                           0.89       194\n",
      "   macro avg       0.94      0.52      0.51       194\n",
      "weighted avg       0.90      0.89      0.84       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import json\n",
    "\n",
    "# Unduh stopwords jika belum\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('dataset/ikn.csv')\n",
    "\n",
    "# Filter hanya data relevan\n",
    "keywords = ['ikn', 'nusantara', 'ibu kota', 'ibukota', 'pemindahan', 'perpindahan']\n",
    "filtered_df = df[df['full_text'].str.contains('|'.join(keywords), case=False, na=False)].copy()\n",
    "filtered_df = filtered_df[filtered_df['full_text'].str.strip() != '']\n",
    "filtered_df = filtered_df.drop_duplicates(subset='full_text')\n",
    "\n",
    "# Inisialisasi Stemmer\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "# Muat kamus kata dasar\n",
    "with open('sastrawi/kata-dasar.txt', 'r', encoding='utf-8') as f:\n",
    "    kamus_sastrawi = set(word.strip() for word in f.readlines())\n",
    "\n",
    "# Kata penting\n",
    "kata_penting = {'ikn', 'nusantara', 'ibu', 'kota', 'ibukota', 'pemindahan', 'perpindahan'}\n",
    "\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "tambahan_stop = {\n",
    "    'gw', 'gue', 'gua', 'lu', 'loe', 'lo', 'elu', 'nya', 'ya', 'aja', 'sih', 'lah', 'deh', 'dong',\n",
    "    'kok', 'nih', 'tuh', 'lagi', 'kayak', 'gak', 'ga', 'nggak', 'ngga', 'yg', 'yang', 'saya', 'kamu'\n",
    "}\n",
    "stop_words.update(tambahan_stop)\n",
    "\n",
    "# Kata sentimen\n",
    "kata_positif = {'baik', 'bagus', 'maju', 'dukung', 'setuju', 'positif', 'indah', 'hebat'}\n",
    "kata_negatif = {'tidak', 'buruk', 'tolak', 'negatif', 'korup', 'jelek', 'hancur', 'bencana', 'rusak'}\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_steps(text):\n",
    "    # Cleaning\n",
    "    data_clean = re.sub(r'http\\S+|#\\w+|@\\w+|\\d+', '', text)\n",
    "    lower = data_clean.lower()\n",
    "    no_punct = lower.translate(str.maketrans('', '', string.punctuation))\n",
    "    replaced = re.sub(r'\\bgw\\b|\\bgue\\b|\\bgua\\b', 'saya', no_punct)\n",
    "    replaced = re.sub(r'\\blu\\b|\\bloe\\b|\\belo\\b|\\belu\\b', 'kamu', replaced)\n",
    "    replaced = re.sub(r'\\bnggak\\b|\\bngga\\b|\\bga\\b|\\bgak\\b', 'tidak', replaced)\n",
    "\n",
    "    tokens = replaced.split()\n",
    "\n",
    "    # Stopword removal\n",
    "    tokens_stop_removed = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    joined_for_stem = ' '.join(tokens_stop_removed)\n",
    "    stemmed_text = stemmer.stem(joined_for_stem)\n",
    "\n",
    "    stemmed_tokens = stemmed_text.split()\n",
    "\n",
    "    # Filter kata dasar atau penting\n",
    "    tokens_filtered = [w for w in stemmed_tokens if w in kamus_sastrawi or w in kata_penting]\n",
    "    tokens_sorted = sorted(tokens_filtered)\n",
    "    final_cleaned = ' '.join(tokens_sorted)\n",
    "\n",
    "    # Deteksi sentimen\n",
    "    sentimen = 'positif' if any(k in tokens_sorted for k in kata_positif) else (\n",
    "        'negatif' if any(k in tokens_sorted for k in kata_negatif) else 'negatif'\n",
    "    )\n",
    "\n",
    "    return pd.Series([final_cleaned, sentimen])\n",
    "\n",
    "# Terapkan preprocessing\n",
    "processed = filtered_df['full_text'].apply(preprocess_steps)\n",
    "processed.columns = ['clean_text', 'sentimen']\n",
    "\n",
    "# Hapus teks pendek\n",
    "processed['jumlah_kata'] = processed['clean_text'].str.split().str.len()\n",
    "processed = processed[processed['jumlah_kata'] >= 3].drop(columns='jumlah_kata')\n",
    "\n",
    "# Tambahkan ID\n",
    "processed.reset_index(drop=True, inplace=True)\n",
    "processed.insert(0, 'id', processed.index + 1)\n",
    "\n",
    "# Simpan hasil preprocessing ke CSV\n",
    "processed.to_csv('dataset/processed.csv', index=False)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed['clean_text'])\n",
    "y = processed['sentimen']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluasi\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['positif', 'negatif'])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"âœ… TF-IDF + Naive Bayes selesai.\")\n",
    "print(f\"ðŸ“Œ Akurasi: {acc * 100:.2f}%\")\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Actual_Pos', 'Actual_Neg'], columns=['Pred_Pos', 'Pred_Neg']))\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d105a8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Jumlah data dan distribusi sentimen setelah preprocessing:\n",
      "sentimen\n",
      "negatif    695\n",
      "positif     80\n",
      "Name: count, dtype: int64\n",
      "Total data latih: 581\n",
      "Distribusi label train: Counter({'negatif': 521, 'positif': 60})\n",
      "\n",
      "ðŸ”¢ Probabilitas Prior (model sklearn):\n",
      "P(negatif) = 0.896730\n",
      "P(positif) = 0.103270\n",
      "\n",
      "âœ… Akurasi: 91.75%\n",
      "\n",
      "ðŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.92      1.00      0.96       174\n",
      "     positif       1.00      0.20      0.33        20\n",
      "\n",
      "    accuracy                           0.92       194\n",
      "   macro avg       0.96      0.60      0.64       194\n",
      "weighted avg       0.92      0.92      0.89       194\n",
      "\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "                Prediksi Positif  Prediksi Negatif\n",
      "Aktual Positif                 4                16\n",
      "Aktual Negatif                 0               174\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Unduh stopwords bahasa Indonesia (jika belum)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset/ikn.csv')\n",
    "\n",
    "# Filter berdasarkan kata kunci tertentu\n",
    "keywords = ['ikn', 'nusantara', 'ibu kota', 'ibukota', 'pemindahan', 'perpindahan']\n",
    "filtered_df = df[df['full_text'].str.contains('|'.join(keywords), case=False, na=False)].copy()\n",
    "filtered_df = filtered_df[filtered_df['full_text'].str.strip() != '']\n",
    "filtered_df = filtered_df.drop_duplicates(subset='full_text')\n",
    "\n",
    "# Inisialisasi stemmer Sastrawi\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "# Muat kamus kata dasar dari file\n",
    "with open('sastrawi/kata-dasar.txt', 'r', encoding='utf-8') as f:\n",
    "    kamus_sastrawi = set(word.strip() for word in f.readlines())\n",
    "\n",
    "# Kata penting yang harus dipertahankan\n",
    "kata_penting = {'ikn', 'nusantara', 'ibu', 'kota', 'ibukota', 'pemindahan', 'perpindahan'}\n",
    "\n",
    "# Stopwords bahasa Indonesia + tambahan slang\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "tambahan_stop = {\n",
    "    'gw', 'gue', 'gua', 'lu', 'loe', 'lo', 'elu', 'nya', 'ya', 'aja', 'sih', 'lah', 'deh', 'dong',\n",
    "    'kok', 'nih', 'tuh', 'lagi', 'kayak', 'gak', 'ga', 'nggak', 'ngga', 'yg', 'yang', 'saya', 'kamu'\n",
    "}\n",
    "stop_words.update(tambahan_stop)\n",
    "\n",
    "# Kata yang berkonotasi positif dan negatif\n",
    "kata_positif = {'baik', 'bagus', 'maju', 'dukung', 'setuju', 'positif', 'indah', 'hebat'}\n",
    "kata_negatif = {'tidak', 'buruk', 'tolak', 'negatif', 'korup', 'jelek', 'hancur', 'bencana', 'rusak'}\n",
    "\n",
    "def preprocess_steps(text):\n",
    "    # Bersihkan teks dari url, tag, angka\n",
    "    data_clean = re.sub(r'http\\S+|#\\w+|@\\w+|\\d+', '', text)\n",
    "    lower = data_clean.lower()\n",
    "    # Hilangkan tanda baca\n",
    "    no_punct = lower.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Ganti kata slang\n",
    "    replaced = re.sub(r'\\bgw\\b|\\bgue\\b|\\bgua\\b', 'saya', no_punct)\n",
    "    replaced = re.sub(r'\\blu\\b|\\bloe\\b|\\belo\\b|\\belu\\b', 'kamu', replaced)\n",
    "    replaced = re.sub(r'\\bnggak\\b|\\bngga\\b|\\bga\\b|\\bgak\\b', 'tidak', replaced)\n",
    "    tokens = replaced.split()\n",
    "    # Stopword removal\n",
    "    tokens_stop_removed = [w for w in tokens if w not in stop_words]\n",
    "    joined_for_stem = ' '.join(tokens_stop_removed)\n",
    "    # Stemming\n",
    "    stemmed_text = stemmer.stem(joined_for_stem)\n",
    "    stemmed_tokens = stemmed_text.split()\n",
    "    # Filter hanya kata dari kamus atau kata penting\n",
    "    tokens_filtered = [w for w in stemmed_tokens if w in kamus_sastrawi or w in kata_penting]\n",
    "    tokens_sorted = sorted(tokens_filtered)\n",
    "    final_cleaned = ' '.join(tokens_sorted)\n",
    "    # Label sentimen berdasarkan kemunculan kata positif atau negatif\n",
    "    sentimen = 'positif' if any(k in tokens_sorted for k in kata_positif) else 'negatif'\n",
    "    return pd.Series([final_cleaned, sentimen])\n",
    "\n",
    "# Preprocessing seluruh data\n",
    "processed = filtered_df['full_text'].apply(preprocess_steps)\n",
    "processed.columns = ['clean_text', 'sentimen']\n",
    "\n",
    "# Filter data dengan minimal 3 kata\n",
    "processed['jumlah_kata'] = processed['clean_text'].str.split().str.len()\n",
    "processed = processed[processed['jumlah_kata'] >= 3].drop(columns='jumlah_kata').reset_index(drop=True)\n",
    "processed.insert(0, 'id', processed.index + 1)\n",
    "\n",
    "# Tampilkan jumlah sentimen setelah preprocessing dan penentuan label\n",
    "print(\"ðŸ“Œ Jumlah data dan distribusi sentimen setelah preprocessing:\")\n",
    "print(processed['sentimen'].value_counts())\n",
    "\n",
    "# Simpan hasil preprocessing jika perlu\n",
    "processed.to_csv('dataset/processed.csv', index=False)\n",
    "\n",
    "# Split data latih dan uji (train 75% test 25%) dengan stratify agar distribusi label sama\n",
    "X = processed['clean_text']\n",
    "y = processed['sentimen']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Debug print data train\n",
    "print(f\"Total data latih: {len(X_train)}\")\n",
    "print(f\"Distribusi label train: {Counter(y_train)}\")\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Latih Multinomial Naive Bayes dengan alpha=1\n",
    "nb = MultinomialNB(alpha=1.0)\n",
    "nb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Tampilkan probabilitas prior dari model sklearn\n",
    "print(\"\\nðŸ”¢ Probabilitas Prior (model sklearn):\")\n",
    "for label, prior_log in zip(nb.classes_, nb.class_log_prior_):\n",
    "    print(f\"P({label}) = {np.exp(prior_log):.6f}\")\n",
    "\n",
    "# Prediksi data test dan evaluasi\n",
    "y_pred = nb.predict(X_test_tfidf)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Tampilkan confusion matrix dengan label yang jelas\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['positif', 'negatif'])\n",
    "cm_df = pd.DataFrame(cm, index=['Aktual Positif', 'Aktual Negatif'], columns=['Prediksi Positif', 'Prediksi Negatif'])\n",
    "\n",
    "print(f\"\\nâœ… Akurasi: {acc * 100:.2f}%\")\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "print(cm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f2d3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ TF-IDF hasil disimpan di 'dataset/tfidf_hasil.csv'\n",
      "   id  abad  abah  abai  abdi  abis  acara  ada  adab  adat  ...  waspada  \\\n",
      "0   1   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "1   2   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "2   3   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "3   4   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "4   5   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "5   6   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "6   7   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "7   8   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "8   9   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "9  10   0.0   0.0   0.0   0.0   0.0    0.0  0.0   0.0   0.0  ...      0.0   \n",
      "\n",
      "   wayang  weh   wilayah  wisata  wujud  yakin  zakat  zalim  ziarah  \n",
      "0     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "1     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "2     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "3     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "4     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "5     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "6     0.0  0.0  0.298769     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "7     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "8     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "9     0.0  0.0  0.000000     0.0    0.0    0.0    0.0    0.0     0.0  \n",
      "\n",
      "[10 rows x 1709 columns]\n",
      "\n",
      "ðŸ“¦ Contoh TF-IDF format dictionary:\n",
      "{\n",
      "  \"dokumen_1\": {\n",
      "    \"baca\": 0.5021190664030798,\n",
      "    \"dalam\": 0.26258266039862843,\n",
      "    \"ikn\": 0.04078431701264515,\n",
      "    \"ilustrasi\": 0.27882359600817963,\n",
      "    \"jaman\": 0.26258266039862843,\n",
      "    \"kabupaten\": 0.24212150135586533,\n",
      "    \"kasih\": 0.22864408043828288,\n",
      "    \"kemarin\": 0.24212150135586533,\n",
      "    \"kendala\": 0.26258266039862843,\n",
      "    \"lokasi\": 0.2510595332015399,\n",
      "    \"mewah\": 0.26258266039862843,\n",
      "    \"paser\": 0.24212150135586533,\n",
      "    \"terima\": 0.24212150135586533,\n",
      "    \"tinggal\": 0.207054534785349\n",
      "  },\n",
      "  \"dokumen_2\": {\n",
      "    \"cabang\": 0.2948700004590402,\n",
      "    \"dinasti\": 0.5897400009180804,\n",
      "    \"dukung\": 0.36031257626239743,\n",
      "    \"ikn\": 0.04313147003486576,\n",
      "    \"jilat\": 0.2483324975756701,\n",
      "    \"kaya\": 0.21897060234963442,\n",
      "    \"kelas\": 0.24180263460132495,\n",
      "    \"lawas\": 0.2948700004590402,\n",
      "    \"masyarakat\": 0.24180263460132495,\n",
      "    \"tau\": 0.22265642695701254,\n",
      "    \"versi\": 0.26550810523300455\n",
      "  },\n",
      "  \"dokumen_3\": {\n",
      "    \"cermin\": 0.3961074056554688,\n",
      "    \"identitas\": 0.43991195969311686,\n",
      "    \"ikn\": 0.06434716816883645,\n",
      "    \"indonesia\": 0.2037038997588536,\n",
      "    \"kota\": 0.21613532407353092,\n",
      "    \"maju\": 0.2667001537393064,\n",
      "    \"nasional\": 0.35230285161782077,\n",
      "    \"satu\": 0.3382009349890902,\n",
      "    \"semangat\": 0.37048338418263116,\n",
      "    \"simbol\": 0.33217765445261915\n",
      "  }\n",
      "}\n",
      "\n",
      "âœ… TF-IDF + Naive Bayes selesai.\n",
      "ðŸ“Œ Akurasi: 88.66%\n",
      "\n",
      "ðŸ“Š Confusion Matrix:\n",
      "            Pred_Pos  Pred_Neg\n",
      "Actual_Pos         1        22\n",
      "Actual_Neg         0       171\n",
      "\n",
      "ðŸ“‹ Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     positif       1.00      0.04      0.08        23\n",
      "     negatif       0.89      1.00      0.94       171\n",
      "\n",
      "    accuracy                           0.89       194\n",
      "   macro avg       0.94      0.52      0.51       194\n",
      "weighted avg       0.90      0.89      0.84       194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Unduh stopwords jika belum\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset/ikn.csv')\n",
    "\n",
    "# Filter relevan\n",
    "keywords = ['ikn', 'nusantara', 'ibu kota', 'ibukota', 'pemindahan', 'perpindahan']\n",
    "filtered_df = df[df['full_text'].str.contains('|'.join(keywords), case=False, na=False)].copy()\n",
    "filtered_df = filtered_df[filtered_df['full_text'].str.strip() != '']\n",
    "filtered_df = filtered_df.drop_duplicates(subset='full_text')\n",
    "\n",
    "# Inisialisasi stemmer dan kata dasar\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "with open('sastrawi/kata-dasar.txt', 'r', encoding='utf-8') as f:\n",
    "    kamus_sastrawi = set(word.strip() for word in f.readlines())\n",
    "\n",
    "# Kata penting, stopwords, dan kata sentimen\n",
    "kata_penting = {'ikn', 'nusantara', 'ibu', 'kota', 'ibukota', 'pemindahan', 'perpindahan'}\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "tambahan_stop = {'gw', 'gue', 'gua', 'lu', 'loe', 'lo', 'elu', 'nya', 'ya', 'aja', 'sih', 'lah', 'deh', 'dong', 'kok', 'nih', 'tuh', 'lagi', 'kayak', 'gak', 'ga', 'nggak', 'ngga', 'yg', 'yang', 'saya', 'kamu'}\n",
    "stop_words.update(tambahan_stop)\n",
    "kata_positif = {'baik', 'bagus', 'maju', 'dukung', 'setuju', 'positif', 'indah', 'hebat'}\n",
    "kata_negatif = {'tidak', 'buruk', 'tolak', 'negatif', 'korup', 'jelek', 'hancur', 'bencana', 'rusak'}\n",
    "\n",
    "# Fungsi preprocessing\n",
    "def preprocess_steps(text):\n",
    "    data_clean = re.sub(r'http\\S+|#\\w+|@\\w+|\\d+', '', text)\n",
    "    lower = data_clean.lower()\n",
    "    no_punct = lower.translate(str.maketrans('', '', string.punctuation))\n",
    "    replaced = re.sub(r'\\bgw\\b|\\bgue\\b|\\bgua\\b', 'saya', no_punct)\n",
    "    replaced = re.sub(r'\\blu\\b|\\bloe\\b|\\belo\\b|\\belu\\b', 'kamu', replaced)\n",
    "    replaced = re.sub(r'\\bnggak\\b|\\bngga\\b|\\bga\\b|\\bgak\\b', 'tidak', replaced)\n",
    "    tokens = replaced.split()\n",
    "    tokens_stop_removed = [w for w in tokens if w not in stop_words]\n",
    "    joined_for_stem = ' '.join(tokens_stop_removed)\n",
    "    stemmed_text = stemmer.stem(joined_for_stem)\n",
    "    stemmed_tokens = stemmed_text.split()\n",
    "    tokens_filtered = [w for w in stemmed_tokens if w in kamus_sastrawi or w in kata_penting]\n",
    "    tokens_sorted = sorted(tokens_filtered)\n",
    "    final_cleaned = ' '.join(tokens_sorted)\n",
    "    sentimen = 'positif' if any(k in tokens_sorted for k in kata_positif) else (\n",
    "        'negatif' if any(k in tokens_sorted for k in kata_negatif) else 'negatif'\n",
    "    )\n",
    "    return pd.Series([final_cleaned, sentimen])\n",
    "\n",
    "# Terapkan preprocessing\n",
    "processed = filtered_df['full_text'].apply(preprocess_steps)\n",
    "processed.columns = ['clean_text', 'sentimen']\n",
    "\n",
    "# Hapus teks pendek\n",
    "processed['jumlah_kata'] = processed['clean_text'].str.split().str.len()\n",
    "processed = processed[processed['jumlah_kata'] >= 3].drop(columns='jumlah_kata')\n",
    "\n",
    "# Tambah ID\n",
    "processed.reset_index(drop=True, inplace=True)\n",
    "processed.insert(0, 'id', processed.index + 1)\n",
    "\n",
    "# Simpan hasil preprocessing\n",
    "processed.to_csv('dataset/processed.csv', index=False)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed['clean_text'])\n",
    "y = processed['sentimen']\n",
    "\n",
    "# Simpan hasil TF-IDF ke CSV\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df.insert(0, 'id', processed['id'])\n",
    "tfidf_df.to_csv('dataset/tfidf_hasil.csv', index=False)\n",
    "print(\"ðŸ“„ TF-IDF hasil disimpan di 'dataset/tfidf_hasil.csv'\")\n",
    "print(tfidf_df.head(10))\n",
    "\n",
    "# ðŸ” Konversi TF-IDF ke format dictionary per dokumen\n",
    "tfidf_dict = {}\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i, row in enumerate(X.toarray()):\n",
    "    dokumen_id = f\"dokumen_{i + 1}\"\n",
    "    tfidf_dict[dokumen_id] = {\n",
    "        feature_names[j]: float(score)\n",
    "        for j, score in enumerate(row) if score > 0\n",
    "    }\n",
    "\n",
    "# Tampilkan 3 dokumen pertama sebagai contoh\n",
    "print(\"\\nðŸ“¦ Contoh TF-IDF format dictionary:\")\n",
    "print(json.dumps(dict(list(tfidf_dict.items())[:3]), indent=2, ensure_ascii=False))\n",
    "\n",
    "# Simpan ke file JSON\n",
    "with open('dataset/tfidf_dict.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tfidf_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "# Evaluasi\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['positif', 'negatif'])\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, labels=['positif', 'negatif'])\n",
    "\n",
    "# Output hasil\n",
    "print(\"\\nâœ… TF-IDF + Naive Bayes selesai.\")\n",
    "print(f\"ðŸ“Œ Akurasi: {acc * 100:.2f}%\")\n",
    "print(\"\\nðŸ“Š Confusion Matrix:\")\n",
    "print(pd.DataFrame(cm, index=['Actual_Pos', 'Actual_Neg'], columns=['Pred_Pos', 'Pred_Neg']))\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
